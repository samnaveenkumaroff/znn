# âš¡ ZNN - Zero Neural Network

> An extensible deep learning and autograd engine built from scratch in Python.

ZNN (Zero Neural Network) is a minimal yet powerful neural network framework designed for learning, experimentation, and customization. It builds a foundation similar to PyTorch's autograd but is developed from scratch with clean architecture, sharp naming, and robust modularity.

---

## ðŸš€ Features

- ðŸ” **Autograd Engine** â€” Pure Python implementation of automatic differentiation
- ðŸ§  **Neural Modules** â€” Neurons, Layers, and MLPs using `Value` nodes
- ðŸ“‰ **Training Loop Templates** â€” Structured optimization and forward/backward passes
- ðŸ› ï¸ **Extensible Design** â€” Easy to modify, expand, or scale
- ðŸŽ¨ **Utilities & Visualization** â€” Clean plots and tooling for learning
- ðŸ§ª **Test Suite** â€” Built-in unit tests for correctness and coverage of program

---

## ðŸ“¦ Installation

### âš¡ Quick Setup (Recommended)

```bash
git clone https://github.com/samnaveenkumaroff/znn.git
cd znn
chmod +x setup.sh
./setup.sh
````

This will:

1. Create and activate a `conda` environment named `znn`
2. Install dependencies: `numpy`, `matplotlib`, `pytest`, `rich`, `jupyter`, etc.
3. Install `znn` in development mode
4. Run tests and XOR demo automatically

### ðŸ§  Manual Setup

```bash
conda create -n znn python=3.10 -y
conda activate znn
pip install -r requirements.txt
pip install -e .
```

---

## ðŸ§ª Run Tests

```bash
pytest tests/
```

---

## ðŸ’¡ Usage Example

```python
from znn import MLP, Value

model = MLP(2, [4, 4, 1])
x = [Value(0.5), Value(-0.2)]
y_true = Value(1.0)

y_pred = model(x)
loss = (y_pred - y_true) ** 2
loss.backward()

# Simple gradient descent
for p in model.parameters():
    p.data -= 0.01 * p.grad

print(f"Loss: {loss.data:.4f}")
```

---

## ðŸ§  Project Structure

```
znn/
â”œâ”€â”€ znn/               # Core package
â”‚   â”œâ”€â”€ engine.py      # Autograd engine (Value class)
â”‚   â”œâ”€â”€ nn.py          # Neural network components
â”‚   â”œâ”€â”€ utils.py       # Utility functions
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ examples/          # Usage demos (e.g. XOR, spiral)
â”‚   â”œâ”€â”€ xor_example.py
â”‚   â””â”€â”€ advanced_example.py
â”œâ”€â”€ tests/             # Unit tests
â”‚   â””â”€â”€ test_engine.py
â”œâ”€â”€ requirements.txt   # Dependencies
â”œâ”€â”€ setup.py           # Package setup
â”œâ”€â”€ environment.yml    # Conda environment
â”œâ”€â”€ setup.sh           # Auto setup script
â””â”€â”€ README.md          # This file
```

---

## ðŸ“š Core Concepts

### âœ… `Value` â€” Differentiable Scalar

```python
from znn import Value

a = Value(2.0)
b = Value(-3.0)
c = a * b + Value(1.0)
c.backward()

print(a.grad, b.grad)  # Gradients via backprop
```

### ðŸ§  Neural Networks

```python
from znn import Neuron, Layer, MLP

mlp = MLP(nin=2, nouts=[4, 4, 1])
output = mlp([Value(0.5), Value(-1.5)])
```

---

## ðŸ” Training Loop Template

```python
for epoch in range(100):
    # Forward pass
    y_pred = model(x)
    loss = (y_pred - y_true)**2

    # Backward pass
    for p in model.parameters():
        p.grad = 0.0
    loss.backward()

    # Update
    for p in model.parameters():
        p.data -= 0.01 * p.grad
```

---

## ðŸŽ¯ Demos

### XOR Training

```bash
python examples/xor_example.py
```

### Spiral Classification

```bash
python examples/advanced_example.py
```

---

## ðŸ›¡ License

Licensed under the MIT License. See [LICENSE](LICENSE) for details.

---

## Inspiration

ZNN was inspired by the clean simplicity of early neural engines. Credit to [micrograd](https://github.com/karpathy/micrograd) for sparking the seed â€” ZNN takes it to the next level.

---

**Made with â¤ï¸ by [Sam Naveenkumar V](https://github.com/samnaveenkumaroff)**

> ZNN â€” Build it. Understand it. Evolve it.

